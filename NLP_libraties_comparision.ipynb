{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expression\n",
    "\n",
    "Python's re Module\n",
    "\n",
    "re module\n",
    "\n",
    "split: split a string on regex\n",
    "\n",
    "findall: find all patterns in a string\n",
    "\n",
    "search: search for a pattern\n",
    "\n",
    "match: match an entire string or substring based on a pattern\n",
    "\n",
    "search can look in the entire word but match uses the start of the word\n",
    "\n",
    "The synthax for the regex library is always to pass the Pattern first, and the string second\n",
    "\n",
    "May return an iterator, string, or match object\n",
    "\n",
    "\n",
    "for the regex library is to always to pass the pattern first, and then the string second.\n",
    "\n",
    "Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument.\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "Turning a string or document into tokens (smaller chunks)\n",
    "\n",
    "One step in preparing a text for NLP\n",
    "\n",
    "Most commond libarary:  nltk\n",
    "\n",
    "Tokenizer libraries:\n",
    "\n",
    "1. sent_tokenize: tokenize a document into sentences. word_tokenize can devide it further to workds. also set(word_tokenize(sentence))  gives the unique words. Here you dont need to use a patters.\n",
    "\n",
    "\n",
    "2. regexp_tokenize: tokenize a string or document based on a regular expression pattern (you need to define a patter)\n",
    "regexp_tokenize(text, pattern)\n",
    "\n",
    "3. TweetTokenizer: special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= open(\"Newsarticles/grail.txt\")\n",
    "scene_one =text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "#print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize. TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='SCENE 1:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w|\\s]+:\"\n",
    "print(re.match(pattern2, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='SCENE 1:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "holy_grail = scene_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:  \n",
    "Please write a short code that counts the number of words in each line of above text and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Split the script into lines: lines \\n devide it into lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker such as ARTHUR: and SOLDIER #1\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line to words: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words\n",
    "- Basic method for finding topics in a text\n",
    "\n",
    "- The more frequent a word, the more important it might be\n",
    "\n",
    "- Can be a great way to determine the significant words in a text\n",
    "\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Import the related package: from collections import Counter\n",
    "\n",
    "2. Tokenize the text\n",
    "\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "1. Make all the words as lowercase: t.lower() for t in tokens\n",
    "\n",
    "2. Lemmatization/Stemming: Shorten words to their root stems\n",
    "\n",
    "3. Removing stop words, punctuation, or unwanted tokens\n",
    "   from ntlk.corpus import stopwords\n",
    "   \n",
    "4. make sure to work just on alphabetical words by  \n",
    "    tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "\n",
    "\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= open(\"Wikipediaarticles/wiki_text_debugging.txt\")\n",
    "article =text.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "It is an open-source NLP library, where uses top academic models to perform complex tasks such as building document or word vectors\n",
    "and performing topic identification and document comparison. We can also biuld a corpora and dictionaries.\n",
    "\n",
    "\n",
    "Computational methodes:\n",
    "1. Preprocessing such as lowecase and tokenizing.\n",
    "2. Use the dictionary of gensim.corpora to provide a dictionary of words.\n",
    "\n",
    "\n",
    "Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "\n",
    "## Corpus\n",
    "Gensim corpus is a list of lists, each list item represeinting one document. \n",
    "\n",
    "corpus = [[(0, 1), (1, 1), ..., (11, 1), (12, 1)]]\n",
    "\n",
    "Each document is a series of tuples, the first item representing the token_id from the dictionary and the second item shows the frequency in the document.\n",
    "\n",
    "Example: We'll use these data structures to investigate word trends and potential interesting topics in the document set.\n",
    "\n",
    "\n",
    "## Example: Analysis of Wikipedia articles:\n",
    "During this excersize we collect all the text article and provide corpus with gensim. Then we use the new gensim corpus and dictionary to see the most common terms per document and across all documents\n",
    "\n",
    "\n",
    "\n",
    " Extra adopted tools:\n",
    " Allows us to initialize a dictionary that will assign a default value to non-existent keys. Defaultdict is a container like dictionaries present in the module collections. Defaultdict is a sub-class of the dict class that returns a dictionary-like object. The functionality of both dictionaries and defualtdict are almost same except for the fact that defualtdict never raises a KeyError. It provides a default value for the key that does not exists.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Not Present\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    " \n",
    "# Function to return a default \n",
    "# values for keys that is not \n",
    "# present \n",
    "def def_value(): \n",
    "    return \"Not Present\"\n",
    "      \n",
    "# Defining the dict \n",
    "d = defaultdict(def_value) \n",
    "d[\"a\"] = 1\n",
    "d[\"b\"] = 2\n",
    "  \n",
    "print(d[\"a\"]) \n",
    "print(d[\"b\"]) \n",
    "print(d[\"c\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  \n",
    "import numpy as np\n",
    "file_list = os.listdir(\"Wikipediaarticles\") \n",
    "lemmatized = []\n",
    "for  file_name in file_list:\n",
    "      with open(os.path.join(\"Wikipediaarticles\", file_name),  encoding=\"utf8\") as src_file:  \n",
    "            tokens = word_tokenize(src_file.read())\n",
    "            \n",
    "            # Convert the tokens into lowercase: lower_tokens\n",
    "            lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "            # Retain alphabetic words: alpha_only\n",
    "            alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "\n",
    "            # Remove all stop words: no_stops\n",
    "            no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "            # Instantiate the WordNetLemmatizer\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "            # Lemmatize all tokens into a new list: lemmatized\n",
    "            lemmatized.append([wordnet_lemmatizer.lemmatize(t) for t in no_stops])\n",
    "\n",
    "  \n",
    "            # data = list.append(lemmatized)\n",
    "    \n",
    "np.size(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (13, 1), (14, 1), (17, 1), (24, 1), (27, 1), (33, 1), (34, 4), (42, 2), (43, 7)]\n",
      "debugging 40\n",
      "system 25\n",
      "bug 17\n",
      "software 16\n",
      "problem 15\n",
      "computer 753\n",
      "software 450\n",
      "program 341\n",
      "cite 322\n",
      "language 320\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from itertools import *\n",
    "import itertools\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(lemmatized)\n",
    "\n",
    "# Create a MmCorpus: corpus from the dictionary\n",
    "corpus = [dictionary.doc2bow(article) for article in lemmatized]\n",
    "\n",
    "\n",
    "print(corpus[4][:10])\n",
    "\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim\n",
    "stands for Term frequency - inverse document frequency. Allows you to determine the most important words in each document. The idea behind tf-idf is that each corpus may have shared words beyond just stopwords. These words should be down-weighted in importance. For example the word of sky in the field of astronomy. If a word is repeated alot in the documant it will have a low weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.012873099301958887), (13, 0.012873099301958887), (14, 0.012873099301958887), (17, 0.012873099301958887), (24, 0.02040337966166453)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the term ids and weights in a new list from highest to lowest weight. This has been done for you.\n",
    "Using your pre-existing dictionary, print the top five weighted words (term_id) from sorted_tfidf_weights, along with their weighted score (weight).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.012873099301958887), (13, 0.012873099301958887), (14, 0.012873099301958887), (17, 0.012873099301958887), (24, 0.02040337966166453)]\n",
      "wolf 0.23074789132791154\n",
      "debugging 0.21371275769012985\n",
      "fence 0.18459831306232924\n",
      "squeeze 0.13844873479674694\n",
      "tron 0.13844873479674694\n"
     ]
    }
   ],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "NLP task to identify important named entities in the text. People, places, organizations. Dates, states, works of art and others... in this process we need to tage a sentence. In this process we need to tag sentences  for parts of speech. This wll add tags for proper nouns, pronouns, advective and verbs. \n",
    "\n",
    "We can see below that this can identify names, verbs and persons names with out consulting a knowlege base, like wikipedia, but it uses the statistical and grammatical parsers.\n",
    "\n",
    "\n",
    " import nltk\n",
    "\n",
    "sentence = '''In New York, I like to ride the Metro to visit MOMA \n",
    "                      and some restaurants rated well by Ruth Reichl.'''\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "\n",
    "tagged_sent[:3]\n",
    "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')\n",
    "\n",
    "\n",
    "Then we pass this sentence to the chunk function\n",
    "\n",
    "print(nltk.ne_chunk(tagged_sent))\n",
    "(S\n",
    "  In/IN\n",
    "  \n",
    "  (GPE New/NNP York/NNP)\n",
    "  \n",
    "  ,/,\n",
    "  \n",
    "  I/PRP\n",
    "  \n",
    "  like/VBP\n",
    "  \n",
    "  to/TO\n",
    "  \n",
    "  ride/VB\n",
    "  \n",
    "  the/DT\n",
    "  \n",
    "  (ORGANIZATION Metro/NNP)\n",
    "  \n",
    "  to/TO\n",
    "  \n",
    "  visit/VB\n",
    "  \n",
    "  (ORGANIZATION MOMA/NNP)\n",
    "  \n",
    "  and/CC\n",
    "  some/DT\n",
    "  restaurants/NNS\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "we see that new and york is taged by NNP which is the tag for the proper noun, and sigular.\n",
    "Extra tools:\n",
    "Inside a list comprehension, tag each tokenized sentence into parts of speech using nltk.pos_tag().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= open(\"Newsarticles/uber_apple.txt\", encoding=\"utf8\")\n",
    "article =text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAACtklEQVR4nO3TMQEAIAzAMMC/52GAnx6Jgj7dM7OAnvM7AHgzJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNC1AVcegTL+uSnUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "NLP library similar to gensim, with different implementations. It Focuses on creating NLP pipelines to generate models and corpora. Open-source, with extra libraries and tools. It has a nice tools with the name of Displacy entity recognition visualizer.\n",
    "\n",
    "(source: https://demos.explosion.ai/displacy-ent/)\n",
    "\n",
    "List of categorie that spacy uses instead of nltk in its named-entity recognition: NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG Uber\n",
      "PERSON Uber\n",
      "ORG unroll.me\n",
      "ORG Apple\n",
      "PERSON Uber\n",
      "PERSON Travis Kalanick\n",
      "PERSON Uber\n",
      "PERSON Tim Cook\n",
      "ORG Apple\n",
      "CARDINAL Millions\n",
      "PERSON Uber\n",
      "LOC Silicon Valley\n",
      "ORG Yahoo\n",
      "PERSON Marissa Mayer\n",
      "MONEY $186m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polyglot\n",
    "The main advantage of this library is that is uses a wide variety of languages it supports. It has more than 130 languages.\n",
    "\n",
    "from polyglot.text import Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
